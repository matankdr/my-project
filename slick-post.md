# Our Slick Journey

## Background
In Pipl, we deal with lots of data and therefore, use multiple kinds of Databases.

[TODO: add background]

### What is Slick
Slick is a Scala library for relational database access and querying. Slick's secret source is that it enables the developer to
work with stored data almost in the same way as working with Scala collections. Which means, is able to write queries in Scala
instead of SQL while gaining full control over database access.

### Why slick?
Softwaremill published a great [post](https://softwaremill.com/comparing-scala-relational-database-access-libraries/) comparing popular Scala libraries for accesing and manipulating data from relational DBs.

The reason we chose Slick over its alternatives is because of its ease of use and its code generation.
As mentioned earlier, instead of writing plain SQL queries the developer works with a Scala collection abstraction over DB tables.
In addition, instead of manually writing the code the models DB columns, Slick is able to auto generate source code by reading the DB schemas.


## Our journey
Our use case for the data monitoring service is as follows:
- Fetch data from multiple tables located in multiple DBs.
- Write new data located in multiple tables, containing relations between them (i.e., foreign keys).
- Update existing data according to logical condition.

[TODO: add description of data-utils use cases]

### First candidate: ScalikeJDBC 
Our existing code base heavily uses [ScalikeJDBC](http://scalikejdbc.org) in various projects.
ScalikeJDBC is a well known Scala library for DB access. ScalikeJDBC wraps plain SQL and JDBC various APIs by a propiatery DSL which
enables the developer to work with type-safe queries.
Its great advantage is its simplicity.

However, one distadvantage is that DB executions created by ScalikeJDBC are blocking. The reason for that is because JDBC drivers performs blocking
on socket IO.
In order to achieve non-blocking DB operations, ScalikeJDBC publish a module for async operations. However, it is still in alpha stage.

In addition, since ScalikeJDBC uses lots of macros under the hood, the IDE is not able to provide autocomplete for such code.
The result is that the developer has to remember/guess what is the correct syntax to work with.
We wanted another tool with better IDE support.

### Enter Slick
We integrated Slick by replacing all existing ScalikeJDBC code base.
Our first use case was to handle queries and data manipulations created from multiple tables located in a single DB.

The key feature was to implement a DAO object that receives a slick DB `profile` instance and from this point,
all data manipulation is executed by the given profile. One of the advantages in this choice of design
is that if we need to work with another type of DB, we just have to inject another `profile` instance,
according to the new DB type.

We used Slick's code generation feature which saved us a lot of time modeling the schema into Scala code [https://scala-slick.org/doc/3.3.3/code-generation.html].
The result was a generated source code containing data presentation read directly from MySQL schema (!).
However, since we had a table containing tens of columns, it drastically increased the project compliation time.
In order to handle this issue, we tried to work with smaller generated source files by splitting Slick generated sources to
multiple files. It was achieved by setting the `outputToMultipleFiles` flag in the Slick code generator.
Compilation time was improved but not heavily.

Finally, the main thing that dramatically improved our compilation time was to manually write table modeling.
We were able to to that since in our use case, we only needed a few columns out of the tens of columns defined in table schema.

### Debugging 
In order to understand what is the exact query executed in DB,
the `named` operator should be applied to the `result`.
In the following example, slick shows the plain SQL query generated by its DSL.

In the following example, we execute a simple data selection from `StatsExpectation` table:

```scala
StatsExpectation
  .filterOpt(request.indexId)(_.indexId === _)
  .filterOpt(request.stageId)(_.stageId === _)
  .filterOpt(request.dsId)(_.dsId === _)
  .result
  .names("stats-expection-query")
```

[TODO: show native query]

### Fallback to native queries
In our development, we had cases where using Slick to execute some queries was either too complicated
or generated native query runtime was too slow.
In these cases, we chose to work with native SQL queries (executed by Slick) and it improved the code 
readability and query performance.

### Testing
Slick provides the developer a teskit that is responsible for managing the full lifecycle of a test, such as:
connecting the DB, getting a list of tables, performing cleanup between tests etc.
In our use use case, we tried working with H2 for testing purposes and found that it does not a perfect fit for our needs.
The reason for that is there are a few data types modeled differently between MySQL and H2 so we wanted to stick with
a single implementation that fits both testing and production code.
We did not work with Slick testkit since we discovered that using a MySQL test container ([https://www.testcontainers.org]) combines the best
of 2 worlds:
- Working with same implementation in both test and production code (in other words, working with a real MySQL instance).
- Relavitely fast cycles (testcontainers library manages the lifecycle of container allocation).
- All development can be done in the developer's machine without the need to connecting remote servers.


## Working with Multi DBs
After delivering the first milestone of the project a new requirement has arrived: data manipulation should be applied to data stored
within multiple DBs.
In this point, we could have extended our DAO constructor to receive multiple profiles.
We found this approach cumbersome, and adopted the cake pattern example located in Slick documentation:

[TODO: add code here]

### Separation of Concerns:
We realized that our project has 2 orthogonal concerns in the project:
- Data located in multiple locations (i.e., multiple DBs)
- Different entities require different data flows 

For example: 
- `Entity1` requires data stored in `DB1` and `DB2`
- `Entity2` requires data stored in `DB2`
- `Entity3` requires data stored in `DB1` and `DB3`

We came up with the following solution:
- Queries related to each DB are stored in a single trait (i.e., `DB1Queries`, `DB2Queries`)
- Each entity has its own DAO (i.e., `Entity1DAO`, `Entity2DAO`)

The above solution enables us flexible design and better testablity of the multiple modules.
Regarding the above use case: `Entity1` contains an instance of `Entity1DAO` with a mixin of `DB1Queries` and `DB2Queries`.
In this way, the developer better understands the concerns relationship of where data is located and what to do with retrieved data.

## Summary
In this post we showed our approach for using Slick's features such as: data modeling, code generation and support of multi-dbs.
We showed that although we integrated Slick to our project, sometimes not doing things in "Slick way" works for us much better.


Slick is not perfect, yet it definitely provides us a great solution for to our needs.

Slick's documentation is lacking some information. 
Useful information can be found in underscore.io book [https://underscore.io/books/essential-slick/]
